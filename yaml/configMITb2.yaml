data:
  root_dir: '/mounts/Datasets3/2024-2025-ChallengePlankton/'
  trainpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/train/'
  testpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/test/'
  batch_size: 64  # MIT-B2 est léger, permet un batch size plus grand
  num_workers: 8
  valid_ratio: 0.1
  patch_size: [320, 320]
  quick_test: False
  transform_type: "fine-details"

optim:
  algo: AdamW
  params:
    lr: 0.0003  # SegFormer aime un LR légèrement plus bas
    weight_decay: 0.0001

scheduler:
  class: CosineAnnealingLR
  params:
    T_max: 50
    eta_min: 1e-6

nepochs: 50
pos_weight: 1

loss:
  name: "TverskyLoss"
  alpha: 0.6  # Optimisé pour préserver les détails fins
  beta: 0.4

logging:
  wandb:
    project: "ChallengePlankton"
    entity: "Random_Predictions_2"
  logdir: "./logs"

model:
  class: Segformer
  encoder:
    model_name: "mit_b2"  # MIT-B2 de smp, bon compromis légèreté/précision
    pretrained: "imagenet"  # Charge les poids pré-entraînés ImageNet
  decoder_segmentation_channels: 256
  in_channels: 1
  classes: 1

test:
  model_path: 'logs/SegFormer_MIT-B2/'
  model_name: 'best_model.pt'
  model_config: 'config.yaml'