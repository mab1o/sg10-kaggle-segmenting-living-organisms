# YOAT GOAT Edition - Optimized for EfficientNetV2-M
data:
  root_dir: '/mounts/Datasets3/2024-2025-ChallengePlankton/'
  trainpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/train/'
  testpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/test/'
  batch_size: 64  # Ajusté pour EfficientNetV2-M, test entre 32 et 48
  num_workers: 8
  valid_ratio: 0.1
  patch_size: [256, 256]  
  quick_test: false
  transform_type: "medium-best"

optim:
  algo: AdamW
  params:
    lr: 0.001  # Meilleur compromis stabilité/rapidité pour EfficientNetV2-M
    weight_decay: 0.001  # Moins de régularisation pour ne pas freiner l'apprentissage

scheduler:
  class: CosineAnnealingWarmRestarts
  params:
    T_0: 3  # Meilleur warm-up
    T_mult: 2
    eta_min: 0.000001

nepochs: 12  
pos_weight: 1

loss:
  name: "TverskyLoss"
  alpha: 0.6
  beta: 0.4

logging:
  wandb:
    project: "ChallengePlankton"
    entity: "Random_Predictions_2"
  logdir: "./logs"

model:
  class: UnetPlusPlus  # FPN est plus efficace pour segmentation fine
  encoder:
    model_name: "tu-efficientnet_b4"  # Format correct pour SMP

# Désactiver si pas de ré-entrainement
# pretrained_model: "logs/UNet_10/best_model.pt"


#changement 1: dice loss, 2: unetplusplus, 3: tu-efficienetnet_b4, 4:medium-best,
#5 : decoder_attention_type="scse"