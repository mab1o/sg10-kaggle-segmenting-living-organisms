data:
  root_dir: '/mounts/Datasets3/2024-2025-ChallengePlankton/'
  trainpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/train/'
  testpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/test/'
  batch_size: 64  
  num_workers: 8
  valid_ratio: 0.1
  patch_size: [256, 256]  
  quick_test: false

optim:
  algo: AdamW
  params:
    lr: 0.0015   # Réduction du LR pour stabiliser l’apprentissage
    weight_decay: 0.0005  # Plus fort pour éviter le sur-apprentissage

scheduler:
  class: CosineAnnealingWarmRestarts
  params:
    T_0: 3  
    T_mult: 2
    eta_min: 1e-6

nepochs: 12  # Augmenté pour stabiliser l’apprentissage avec les nouvelles transfo
pos_weight: 1

loss:
  name: "Tversky-BCE"
  alpha: 0.6 
  beta: 0.4
  
logging:
  wandb:
    project: "ChallengePlankton"
    entity: "Random_Predictions_2"
  logdir: "./logs"


model:
  class: EfficientNetB3Segmentation
  input_channels: 1  # Modify based on your input data
  num_classes: 1     # Binary segmentation

#désactiver si pas de ré-entrainement
#pretrained_model: "logs/UNet_10/best_model.pt"
#current best is UNet_10.

test:
  model_path: 'logs/UNet_10/'
  model_name: 'best_model.pt'
  model_config: 'config.yaml'
