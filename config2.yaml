data:
  root_dir:  '/mounts/Datasets3/2024-2025-ChallengePlankton/'
  trainpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/train/'
  testpath: '/mounts/Datasets3/2024-2025-ChallengePlankton/test/'
  batch_size: 48
  num_workers: 8
  valid_ratio: 0.15 #changed from 0.2 to 0.1 because the dataset is quite large
  patch_size: [300,300]
  quick_test: false
optim:
  algo: AdamW
  params:
    lr: 0.001
    weight_decay: 0.01
scheduler:
  class: CosineAnnealingWarmRestarts  # Switched to WarmRestarts for dynamic adaptation
  params:
    T_0: 4  # Restart every 4 epochs for better exploration
    T_mult: 1  # Consistent restarts
    eta_min: 1e-6  # Maintain a very small LR at decay phases

nepochs: 12
# loss: "CrossEntropyLoss", "BCEWithLogitsLoss"
#pos_weight: 19.63  # Calcul√© comme (num_negatives / num_positives) avec environ 5% de positif. Put at 1 to desactivate
pos_weight: 1 #reduced the value to avoid instability
loss: 
  name: "TverskyLoss"
  alpha: 0.4
  beta: 0.6
# standard is alpha=0.3, beta=0.7
logging:
  wandb: 
    project: "ChallengePlankton"
    entity: "Random_Predictions_2"  # Remplace par ton nom d'utilisateur ou celui de ton organisation
  logdir: "./logs"  # Emplacement des logs locaux

model:
  class: EfficientNetB3Segmentation
  input_channels: 1  # Modify based on your input data
  num_classes: 1     # Binary segmentation

test:
  model_path: 'logs/EfficientNetB3Segmentation_4/'
  model_name: 'best_model.pt'
  model_config: 'config.yaml'


